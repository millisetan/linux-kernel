REFERENCE
tcp: https://tools.ietf.org/html/rfc793
sack: https://tools.ietf.org/html/rfc2018
fack: http://conferences.sigcomm.org/sigcomm/1996/papers/mathis.pdf
dsack: https://tools.ietf.org/html/rfc2883
rack: https://tools.ietf.org/html/draft-ietf-tcpm-rack-06
Early Retransmit: https://tools.ietf.org/html/rfc5827
PAWS: https://tools.ietf.org/html/rfc7323 
TCP Timestamps Option: https://tools.ietf.org/html/rfc7323
TCP Window Scale: https://tools.ietf.org/html/rfc7323
nagle small-packet problem: https://tools.ietf.org/html/rfc896
    With the Nagle algorithm, a first small packet will be transmitted, then subsequent writes from the application will be buffered at the sending TCP until either i) enough application data has accumulated to enable TCP to transmit a maximum sized packet, or ii) the initial small packet is acknowledged by the receiving TCP.
minshall A Suggested Modification to Nagle's Algorithm: https://tools.ietf.org/html/draft-minshall-nagle-00
    If a TCP has less than a full-sized packet to transmit, and if any previous less than full-sized packet has not yet been acknowledged, do not transmit a packet.
delayed Ack: https://tools.ietf.org/html/rfc1122
    the delay MUST be less than 0.5 seconds, and in a stream of full-sized segments there SHOULD be an ACK for at least every second segment.
    

VARIABLE EXPLAIN
tcp_rtx_queue: retransmit queue


TCP OPTION
#define OPTION_SACK_ADVERTISE	(1 << 0)
#define OPTION_TS		(1 << 1)
#define OPTION_MD5		(1 << 2)
#define OPTION_WSCALE		(1 << 3)
#define OPTION_FAST_OPEN_COOKIE	(1 << 8)
#define OPTION_SMC		(1 << 9)


/*
inetsw_array

tcp
sock->sk->sk_prot = tcp_prot,
sock->ops = inet_stream_ops,
inet_csk(sock->sk)->icsk_af_ops = ipv4_specific

backlog_rcv		= tcp_v4_do_rcv

static struct net_protocol tcp_protocol = {
	.early_demux	=	tcp_v4_early_demux,
	.early_demux_handler =  tcp_v4_early_demux,
	.handler	=	tcp_v4_rcv,
	.err_handler	=	tcp_v4_err,
	.no_policy	=	1,
	.netns_ok	=	1,
	.icmp_strict_tag_validation = 1,
};

*/


int tcp_v4_rcv(struct sk_buff *skb)


int __sys_socket(int family, int type, int protocol)
sock_create(family, type, protocol, &sock);
		int sock_create(int family, int type, int protocol, struct socket **res)
	pf = rcu_dereference(net_families[family]);
	err = pf->create(net, sock, protocol, kern);
				static int inet_create(struct net *net, struct socket *sock, int protocol, int kern)
				list_for_each_entry_rcu(answer, &inetsw[sock->type], list)
				answer_prot = answer->prot;
				sk = sk_alloc(net, PF_INET, GFP_KERNEL, answer_prot, kern);
				sock_init_data(sock, sk);
				sk->sk_prot->init(sk);
						static int tcp_v4_init_sock(struct sock *sk)
						void tcp_init_sock(struct sock *sk)


int __sys_listen(int fd, int backlog)
sock = sockfd_lookup_light(fd, &err, &fput_needed);
err = sock->ops->listen(sock, backlog);
int inet_listen(struct socket *sock, int backlog)
		err = inet_csk_listen_start(sk, backlog);
		inet_sk_state_store(sk, TCP_LISTEN);
		sk->sk_prot->get_port(sk, inet->inet_num)
				int inet_csk_get_port(struct sock *sk, unsigned short snum)


int __sys_bind(int fd, struct sockaddr __user *umyaddr, int addrlen)
err = sock->ops->bind(sock, (struct sockaddr *) &address, addrlen);
		int inet_bind(struct socket *sock, struct sockaddr *uaddr, int addr_len)
	__inet_bind(sk, uaddr, addr_len, false, true);
				int __inet_bind(struct sock *sk, struct sockaddr *uaddr, int addr_len,
		sk->sk_prot->get_port(sk, snum)


int __sys_connect(int fd, struct sockaddr __user *uservaddr, int addrlen)
	int inet_stream_connect(struct socket *sock, struct sockaddr *uaddr,
		int tcp_v4_connect(struct sock *sk, struct sockaddr *uaddr, int addr_len)
			bool tcp_fastopen_defer_connect(struct sock *sk, int *err)
			int tcp_connect(struct sock *sk)
			err = tp->fastopen_req ? tcp_send_syn_data(sk, buff) : tcp_transmit_skb(sk, buff, 1, sk->sk_allocation);
				int tcp_send_syn_data(struct sock *sk, struct sk_buff *syn)
				int tcp_transmit_skb(struct sock *sk, struct sk_buff *skb, int clone_it,


int tcp_v4_do_rcv(struct sock *sk, struct sk_buff *skb)
    if (sk->sk_state == TCP_ESTABLISHED) { /* Fast path */
        tcp_rcv_established(sk, skb);
	if (sk->sk_state == TCP_LISTEN) {
        struct sock *nsk = tcp_v4_cookie_check(sk, skb);
            cookie_v4_check(sk, skb);
                tcp_get_cookie_sock(sk, skb, req, &rt->dst, tsoff);
                    icsk->icsk_af_ops->syn_recv_sock(sk, skb, req, dst,
                        struct sock *tcp_v4_syn_recv_sock(const struct sock *sk, struct sk_buff *skb,
                    inet_csk_reqsk_queue_add(sk, req, child)
        tcp_child_process(sk, nsk, skb)
            if (!sock_owned_by_user(child)) {
                ret = tcp_rcv_state_process(child, skb);
                if (state == TCP_SYN_RECV && child->sk_state != state)
                    parent->sk_data_ready(parent);
            } else {
                __sk_add_backlog(child, skb);
            }


//RFC 793 main procedure
int tcp_rcv_state_process(struct sock *sk, struct sk_buff *skb)
case TCP_LISTEN:
    acceptable = icsk->icsk_af_ops->conn_request(sk, skb) >= 0;
case TCP_SYN_SENT:
    queued = tcp_rcv_synsent_state_process(sk, skb, th);
		tcp_ack(sk, skb, FLAG_SLOWPATH);
		tcp_finish_connect(sk, skb);


int __sys_accept4(int fd, struct sockaddr __user *upeer_sockaddr,
sock = sockfd_lookup_light(fd, &err, &fput_needed);
err = sock->ops->accept(sock, newsock, sock->file->f_flags, false);
		int inet_accept(struct socket *sock, struct socket *newsock, int flags,
	struct sock *sk2 = sk1->sk_prot->accept(sk1, flags, &err, kern);
				struct sock *inet_csk_accept(struct sock *sk, int flags, int *err, bool kern)
		error = inet_csk_wait_for_connect(sk, timeo);


int __sys_sendto(int fd, void __user *buff, size_t len, unsigned int flags,
err = sock_sendmsg(sock, &msg);
	int sock_sendmsg(struct socket *sock, struct msghdr *msg)
	sock->ops->sendmsg(sock, msg, msg_data_left(msg));
				int inet_sendmsg(struct socket *sock, struct msghdr *msg, size_t size)
				sk->sk_prot->sendmsg(sk, msg, size);
						int tcp_sendmsg(struct sock *sk, struct msghdr *msg, size_t size)
						ret = tcp_sendmsg_locked(sk, msg, size);

long __sys_recvmsg(int fd, struct user_msghdr __user *msg, unsigned int flags,
err = ___sys_recvmsg(sock, msg, &msg_sys, flags, 0);
err = (nosec ? sock_recvmsg_nosec : sock_recvmsg)(sock, msg_sys, flags);
		int inet_recvmsg(struct socket *sock, struct msghdr *msg, size_t size,
		 int flags)
		int tcp_recvmsg(struct sock *sk, struct msghdr *msg, size_t len, int nonblock,

/* tcp quick ack */
void tcp_enter_quickack_mode(struct sock *sk, unsigned int max_quickacks);
void tcp_dec_quickack_mode(struct sock *sk, const unsigned int pkts)
