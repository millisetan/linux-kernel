REFERENCE
tcp: https://tools.ietf.org/html/rfc793
sack: https://tools.ietf.org/html/rfc2018
fack: http://conferences.sigcomm.org/sigcomm/1996/papers/mathis.pdf
dsack: https://tools.ietf.org/html/rfc2883
PAWS: https://tools.ietf.org/html/rfc7323 
TCP Timestamps Option: https://tools.ietf.org/html/rfc7323
TCP Window Scale: https://tools.ietf.org/html/rfc7323

Silly Window Syndrome: http://www.tcpipguide.com/free/t_TCPSillyWindowSyndromeandChangesTotheSlidingWindow.htm

NewReno: https://tools.ietf.org/html/rfc6582
    In typical implementation, data sender only retransmits a packet after a retransmit timeout has occurred, or after three duplicate acknowledgments have arrived triggering the fast retransmit algorithm. In NewReno, While we are in Recovery and a partial ACK arrives, we assume that one more packet is lost.

nagle small-packet problem: https://tools.ietf.org/html/rfc896
    With the Nagle algorithm, a first small packet will be transmitted, then subsequent writes from the application will be buffered at the sending TCP until either i) enough application data has accumulated to enable TCP to transmit a maximum sized packet, or ii) the initial small packet is acknowledged by the receiving TCP.

minshall A Suggested Modification to Nagle's Algorithm: https://tools.ietf.org/html/draft-minshall-nagle-00
    If a TCP has less than a full-sized packet to transmit, and if any previous less than full-sized packet has not yet been acknowledged, do not transmit a packet.

delayed Ack: https://tools.ietf.org/html/rfc1122
    the delay MUST be less than 0.5 seconds, and in a stream of full-sized segments there SHOULD be an ACK for at least every second segment.
MSS The TCP Maximum Segment Size and Related Topics: https://tools.ietf.org/html/rfc879

TCP Fast Open: https://tools.ietf.org/html/rfc7413
    tcp_fastopen_defer_connect
    MSG_FASTOPEN
    
SYN cookies: http://cr.yp.to/syncookies.html
    SYN cookies are particular choices of initial TCP sequence numbers by TCP servers

TLP An Algorithm for Fast Recovery of Tail Losses: https://tools.ietf.org/html/draft-dukkipati-tcpm-tcp-loss-probe-01
    if we are at the tail of sent, we can't wait the long and conservative RTO to fire retransmit, it hurts performance(latency, bindwidth), instead we schedule a short and aggressive PTO timer to fire retransmit. PTO retransmit may prevent tcp detect lost because retransmit may mask the original packet's lost, we must take the PTO retransmit into account when detect some packet is lost.
    cause: drops at the tail end of transactions or a loss of an entire window of data/ACKs
    intention: Tail Loss Probe (TLP), which sends probe segments to trigger duplicate ACKs with the intent of invoking fast recovery more quickly than an RTO at the end of a transaction
    linux kernel patch: https://lwn.net/Articles/542642/

Early Retransmit: https://tools.ietf.org/html/rfc5827
    fast retransmit use 3 dupack to detect loss (3 is chosen to distinguish reorder from loss), when reciever can not generate 3 dupack(outstanding seg < 4 && no new data can be sent), we need to early retransmit, if all packet except the one after SND_UNACK is sack, we deem that packet is lost.
    ER_thresh = 3   // 等于3，表示还是标准的FR算法
    if (oseg < 4 && new data cannot be sent)    // 如果满足条件，考虑启用ER算法
        if (SACK is unsupport)                  // 如果SACK选项不支持，则使用oseg-1作为阈值
            ER_thresh = oseg - 1
        elif (SACKed packet == oseg-1)          // 否则，只有当oseg-1个包被SACK，才能启用ER
            ER_thresh = oseg - 1

rack: https://tools.ietf.org/html/draft-ietf-tcpm-rack-06
    RACK uses the most recently delivered packet’s transmission time to judge if some packets sent previous to that time have "expired" by passing a certain reordering settling window(if a packet arrives now, we calculate the reordering windows is 20ms, we deem all packet sent before that packet by 20ms earlier is lost).
    
Forward RTO-Recovery (F-RTO): https://tools.ietf.org/html/rfc5682
    The main motivation of the algorithm is to recover efficiently from a spurious RTO.

A Conservative Loss Recovery Algorithm Based on Selective Acknowledgment (SACK) for TCP: https://tools.ietf.org/html/rfc6675
    sequence number S is lost when DupThresh sack arrived after s or DupThresh * MSS byte sack after S

Proportional Rate Reduction for TCP(PRR): https://tools.ietf.org/html/rfc6937
    Van Jacobson's packet conservation principle: segments delivered to the receiver are used as the clock to trigger sending the same number of segments back into the network.
    total sends eqaul to delivered when tcp_packets_in_flight < ssthresh, else total sends eqaul to  proportional(ssthresh/RecoverFS) of delivered
    

VARIABLE EXPLAIN
tcp_rtx_queue: retransmit queue
CWR: Congestion Window Reduced


TCP OPTION
#define OPTION_SACK_ADVERTISE	(1 << 0)
#define OPTION_TS		(1 << 1)
#define OPTION_MD5		(1 << 2)
#define OPTION_WSCALE		(1 << 3)
#define OPTION_FAST_OPEN_COOKIE	(1 << 8)
#define OPTION_SMC		(1 << 9)


/*
inetsw_array

tcp
sock->sk->sk_prot = tcp_prot,
sock->ops = inet_stream_ops,
inet_csk(sock->sk)->icsk_af_ops = ipv4_specific
tcp_request_sock_ipv4_ops

backlog_rcv		= tcp_v4_do_rcv

static struct net_protocol tcp_protocol = {
	.early_demux	=	tcp_v4_early_demux,
	.early_demux_handler =  tcp_v4_early_demux,
	.handler	=	tcp_v4_rcv,
	.err_handler	=	tcp_v4_err,
	.no_policy	=	1,
	.netns_ok	=	1,
	.icmp_strict_tag_validation = 1,
};

*/


int tcp_v4_rcv(struct sk_buff *skb)


int __sys_socket(int family, int type, int protocol)
sock_create(family, type, protocol, &sock);
		int sock_create(int family, int type, int protocol, struct socket **res)
	pf = rcu_dereference(net_families[family]);
	err = pf->create(net, sock, protocol, kern);
				static int inet_create(struct net *net, struct socket *sock, int protocol, int kern)
				list_for_each_entry_rcu(answer, &inetsw[sock->type], list)
				answer_prot = answer->prot;
				sk = sk_alloc(net, PF_INET, GFP_KERNEL, answer_prot, kern);
				sock_init_data(sock, sk);
				sk->sk_prot->init(sk);
						static int tcp_v4_init_sock(struct sock *sk)
						void tcp_init_sock(struct sock *sk)


int __sys_listen(int fd, int backlog)
sock = sockfd_lookup_light(fd, &err, &fput_needed);
err = sock->ops->listen(sock, backlog);
int inet_listen(struct socket *sock, int backlog)
		err = inet_csk_listen_start(sk, backlog);
		inet_sk_state_store(sk, TCP_LISTEN);
		sk->sk_prot->get_port(sk, inet->inet_num)
				int inet_csk_get_port(struct sock *sk, unsigned short snum)


int __sys_bind(int fd, struct sockaddr __user *umyaddr, int addrlen)
err = sock->ops->bind(sock, (struct sockaddr *) &address, addrlen);
		int inet_bind(struct socket *sock, struct sockaddr *uaddr, int addr_len)
	__inet_bind(sk, uaddr, addr_len, false, true);
				int __inet_bind(struct sock *sk, struct sockaddr *uaddr, int addr_len,
		sk->sk_prot->get_port(sk, snum)


int __sys_connect(int fd, struct sockaddr __user *uservaddr, int addrlen)
	int inet_stream_connect(struct socket *sock, struct sockaddr *uaddr,
		int tcp_v4_connect(struct sock *sk, struct sockaddr *uaddr, int addr_len)
			bool tcp_fastopen_defer_connect(struct sock *sk, int *err)
			int tcp_connect(struct sock *sk)
			err = tp->fastopen_req ? tcp_send_syn_data(sk, buff) : tcp_transmit_skb(sk, buff, 1, sk->sk_allocation);
				int tcp_send_syn_data(struct sock *sk, struct sk_buff *syn)
				int tcp_transmit_skb(struct sock *sk, struct sk_buff *skb, int clone_it,


int tcp_v4_do_rcv(struct sock *sk, struct sk_buff *skb)
    if (sk->sk_state == TCP_ESTABLISHED) { /* Fast path */
        tcp_rcv_established(sk, skb);
	if (sk->sk_state == TCP_LISTEN)
        //SYN cookies, 3WHS completed, check and delayed create
        struct sock *nsk = tcp_v4_cookie_check(sk, skb);
            cookie_v4_check(sk, skb);
                tcp_get_cookie_sock(sk, skb, req, &rt->dst, tsoff);
                    icsk->icsk_af_ops->syn_recv_sock(sk, skb, req, dst,
                        struct sock *tcp_v4_syn_recv_sock(const struct sock *sk, struct sk_buff *skb,
                    inet_csk_reqsk_queue_add(sk, req, child)
        tcp_child_process(sk, nsk, skb)
            if (!sock_owned_by_user(child)) {
                ret = tcp_rcv_state_process(child, skb);
                if (state == TCP_SYN_RECV && child->sk_state != state)
                    parent->sk_data_ready(parent);
            } else {
                __sk_add_backlog(child, skb);
            }
	tcp_rcv_state_process(sk, skb)


//RFC 793 main procedure
int tcp_rcv_state_process(struct sock *sk, struct sk_buff *skb)
case TCP_LISTEN:
    acceptable = icsk->icsk_af_ops->conn_request(sk, skb) >= 0;
        int tcp_v4_conn_request(struct sock *sk, struct sk_buff *skb)
        return tcp_conn_request(&tcp_request_sock_ops, &tcp_request_sock_ipv4_ops, sk, skb);
            isn = cookie_init_sequence(af_ops, sk, skb, &req->mss);
            tcp_try_fastopen(sk, skb, req, &foc, dst);
                tcp_fastopen_queue_check(sk))
                tcp_fastopen_cookie_gen(sk, req, skb, &valid_foc)
                tcp_fastopen_create_child(sk, skb, req);
                    inet_csk(sk)->icsk_af_ops->syn_recv_sock(sk, skb, req, NULL, NULL, &own_req);
                    tcp_v4_syn_recv_sock(const struct sock *sk, struct sk_buff *skb,...)
                        newsk = tcp_create_openreq_child(sk, req, skb);
            af_ops->send_synack(sk, dst, &fl, req, &foc, ...)
                static int tcp_v4_send_synack(const struct sock *sk, struct dst_entry *dst, ...)
case TCP_SYN_SENT:
    queued = tcp_rcv_synsent_state_process(sk, skb, th);
		tcp_ack(sk, skb, FLAG_SLOWPATH);
		tcp_finish_connect(sk, skb);
            tcp_set_state(sk, TCP_ESTABLISHED);
            tcp_init_transfer(sk, BPF_SOCK_OPS_ACTIVE_ESTABLISHED_CB);
    tcp_urg(sk, skb, th);
        tcp_check_urg(sk, th);
    tcp_data_snd_check(sk);
        tcp_push_pending_frames(sk);
        tcp_check_space(sk);


void tcp_rcv_established(struct sock *sk, struct sk_buff *skb)


int __sys_accept4(int fd, struct sockaddr __user *upeer_sockaddr,
sock = sockfd_lookup_light(fd, &err, &fput_needed);
err = sock->ops->accept(sock, newsock, sock->file->f_flags, false);
		int inet_accept(struct socket *sock, struct socket *newsock, int flags,
	struct sock *sk2 = sk1->sk_prot->accept(sk1, flags, &err, kern);
				struct sock *inet_csk_accept(struct sock *sk, int flags, int *err, bool kern)
		error = inet_csk_wait_for_connect(sk, timeo);


int __sys_sendto(int fd, void __user *buff, size_t len, unsigned int flags,
err = sock_sendmsg(sock, &msg);
	int sock_sendmsg(struct socket *sock, struct msghdr *msg)
	sock->ops->sendmsg(sock, msg, msg_data_left(msg));
				int inet_sendmsg(struct socket *sock, struct msghdr *msg, size_t size)
				sk->sk_prot->sendmsg(sk, msg, size);
						int tcp_sendmsg(struct sock *sk, struct msghdr *msg, size_t size)
						ret = tcp_sendmsg_locked(sk, msg, size);

long __sys_recvmsg(int fd, struct user_msghdr __user *msg, unsigned int flags,
err = ___sys_recvmsg(sock, msg, &msg_sys, flags, 0);
err = (nosec ? sock_recvmsg_nosec : sock_recvmsg)(sock, msg_sys, flags);
		int inet_recvmsg(struct socket *sock, struct msghdr *msg, size_t size,
		 int flags)
		int tcp_recvmsg(struct sock *sk, struct msghdr *msg, size_t len, int nonblock,

/* tcp quick ack */
void tcp_enter_quickack_mode(struct sock *sk, unsigned int max_quickacks);
void tcp_dec_quickack_mode(struct sock *sk, const unsigned int pkts)
